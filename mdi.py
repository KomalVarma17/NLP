# -*- coding: utf-8 -*-
"""MDI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g88Fs0iAfe9HcRab0IbFPE3VFOS3Rj37

Created dataset for medical terms and their explanations using web scraping techniques on the medline plus website.
created a manual dictionary and saved it to a csv file which has medical abbreviations and their full forms.
Loaded the datasets required.(medical dataset which has transcriptions, and the created datasets.)
applied pre processing tasks on both medical terms dataset and dataset with transcriptions.
used biobert,clinical bert models and tried to extract medical entities from the transcriptions.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd


data = []


base_url = "https://medlineplus.gov/ency/encyclopedia_{}.htm"
letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'


desired_headers = ['Symptoms', 'Treatment', 'Causes', 'Prevention', 'Possible Complications', 'Exams and Tests',
                   'How the test is performed', 'How to Prepare for the Test', 'Why the Test is Performed',
                   'Risks', 'Alternative Names', 'Normal Results']


for letter in letters:

    url = base_url.format(letter)
    response = requests.get(url)

    if response.status_code != 200:
        print(f"Failed to retrieve data for letter: {letter}")
        continue


    soup = BeautifulSoup(response.content, "html.parser")


    ul_index = soup.find('ul', id='index')
    if not ul_index:
        print(f"No terms found for letter: {letter}")
        continue


    terms = ul_index.find_all('li')


    for term in terms:
        link = term.find('a')
        if link:
            term_name = link.text.strip()
            term_link = "https://medlineplus.gov/ency/" + link['href']


            term_response = requests.get(term_link)
            term_soup = BeautifulSoup(term_response.content, "html.parser")


            summary_div = term_soup.find('div', id='ency_summary')
            summary_texts = []
            if summary_div:
                paragraphs = summary_div.find_all('p')
                summary_texts.extend(p.get_text(strip=True) for p in paragraphs)

                list_items = summary_div.find_all('li')
                summary_texts.extend(li.get_text(strip=True) for li in list_items)


            sections = term_soup.find_all('section')
            section_texts = []
            for section in sections:
                header_div = section.find('div', class_='section-header')
                if header_div:
                    header_title = header_div.find('h2')
                    if header_title and header_title.get_text(strip=True) in desired_headers:
                        section_title = header_title.get_text(strip=True)
                        section_texts.append(f"Section Title: {section_title}")

                        paragraphs = section.find_all('p')
                        section_texts.extend(p.get_text(strip=True) for p in paragraphs)

                        list_items = section.find_all('li')
                        section_texts.extend(li.get_text(strip=True) for li in list_items)


            explanation = "\n".join(summary_texts + section_texts)


            data.append([term_name, term_link, explanation])


df = pd.DataFrame(data, columns=['Medical Term', 'Link to Article', 'Explanation'])


print(df)


df.to_csv("medlineplus_medical_terms.csv", index=False)

df.to_csv("/content/drive/MyDrive/NLP/medlineplus_medical_terms.csv", index=False)

import pandas as pd


unique_abbreviations = {
    'aa': 'aplastic anemia',
    'aaa': 'abdominal aortic aneurysm',
    'aaox3': 'awake, alert, and oriented to person, place, and time',
    'ac': 'before meals',
    'acl': 'anterior cruciate ligament',
    'ad': 'right ear',
    'adls': 'activities of daily living',
    'adm': 'admission',
    'af': 'atrial fibrillation',
    'afib': 'atrial fibrillation',
    'afp': 'alpha-fetoprotein',
    'alb': 'albumin',
    'alc': 'alcohol',
    'amb': 'ambulate',
    'amt': 'amount',
    'ap': 'anteroposterior',
    'apo': 'apolipoprotein',
    'app': 'appendectomy',
    'apr': 'abdominoperineal resection',
    'as': 'left ear',
    'asa': 'acetylsalicylic acid (aspirin)',
    'ashd': 'arteriosclerotic heart disease',
    'au': 'both ears',
    'av': 'atrioventricular',
    'avf': 'arteriovenous fistula',
    'avm': 'arteriovenous malformation',
    'bka': 'below-knee amputation',
    'bmp': 'basic metabolic panel',
    'bn': 'bulimia nervosa',
    'bp': 'blood pressure',
    'bpm': 'beats per minute',
    'bsa': 'body surface area',
    'bso': 'bilateral salpingo-oophorectomy',
    'bx': 'biopsy',
    'ca': 'cancer',
    'cabg': 'coronary artery bypass graft',
    'cad': 'coronary artery disease',
    'caf': 'cardiac arrest failure',
    'cai': 'coronary artery insufficiency',
    'cap': 'community-acquired pneumonia',
    'cath': 'catheterization',
    'cbc': 'complete blood count',
    'cc': 'chief complaint',
    'ccu': 'cardiac care unit',
    'cd': "Crohn's disease",
    'cf': 'cystic fibrosis',
    'cfs': 'chronic fatigue syndrome',
    'chf': 'congestive heart failure',
    'cml': 'chronic myeloid leukemia',
    'cns': 'central nervous system',
    'copd': 'chronic obstructive pulmonary disease',
    'cp': 'chest pain',
    'cpr': 'cardiopulmonary resuscitation',
    'cr': 'creatinine',
    'crf': 'chronic renal failure',
    'crt': 'capillary refill time',
    'csf': 'cerebrospinal fluid',
    'ct': 'computed tomography',
    'cv': 'cardiovascular',
    'cva': 'cerebrovascular accident (stroke)',
    'cvc': 'central venous catheter',
    'cxr': 'chest x-ray',
    'da': 'dental assistant',
    'dka': 'diabetic ketoacidosis',
    'dm': 'diabetes mellitus',
    'dmd': 'doctor of dental medicine',
    'dme': 'durable medical equipment',
    'dnr': 'do not resuscitate',
    'doa': 'dead on arrival',
    'dob': 'date of birth',
    'dt': 'delirium tremens',
    'dvt': 'deep vein thrombosis',
    'dx': 'diagnosis',
    'ecg': 'electrocardiogram',
    'echo': 'echocardiogram',
    'ed': 'emergency department',
    'ef': 'ejection fraction',
    'ekg': 'electrocardiogram',
    'emt': 'emergency medical technician',
    'epo': 'erythropoietin',
    'esr': 'erythrocyte sedimentation rate',
    'etoh': 'alcohol',
    'fbs': 'fasting blood sugar',
    'fhr': 'fetal heart rate',
    'fvc': 'forced vital capacity',
    'fx': 'fracture',
    'g6pd': 'glucose-6-phosphate dehydrogenase',
    'ga': 'gestational age',
    'gb': 'gallbladder',
    'gcs': 'Glasgow Coma Scale',
    'gi': 'gastrointestinal',
    'gtt': 'glucose tolerance test',
    'hct': 'hematocrit',
    'hgb': 'hemoglobin',
    'hiv': 'human immunodeficiency virus',
    'hpi': 'history of present illness',
    'hr': 'heart rate',
    'htn': 'hypertension',
    'i&o': 'intake and output',
    'ibd': 'inflammatory bowel disease',
    'icd': 'implantable cardioverter defibrillator',
    'icu': 'intensive care unit',
    'iddm': 'insulin-dependent diabetes mellitus',
    'im': 'intramuscular',
    'iv': 'intravenous',
    'k': 'potassium',
    'kcal': 'kilocalories',
    'kg': 'kilogram',
    'ldl': 'low-density lipoprotein',
    'lh': 'luteinizing hormone',
    'lmp': 'last menstrual period',
    'lpn': 'licensed practical nurse',
    'lsp': 'lumbar spine',
    'mci': 'mild cognitive impairment',
    'mcv': 'mean corpuscular volume',
    'mi': 'myocardial infarction',
    'mmr': 'measles, mumps, rubella',
    'ms': 'multiple sclerosis',
    'mva': 'motor vehicle accident',
    'npo': 'nil per os (nothing by mouth)',
    'ns': 'normal saline',
    'nsaids': 'non-steroidal anti-inflammatory drugs',
    'nvd': 'nausea, vomiting, diarrhea',
    'o2': 'oxygen',
    'od': 'right eye',
    'os': 'left eye',
    'ou': 'both eyes',
    'pa': 'posterior-anterior',
    'pac': 'premature atrial contraction',
    'pap': 'papanicolaou test',
    'pcp': 'primary care physician',
    'pda': 'patent ductus arteriosus',
    'pe': 'physical examination',
    'pft': 'pulmonary function test',
    'pku': 'phenylketonuria',
    'plts': 'platelets',
    'pmi': 'point of maximal impulse',
    'po': 'by mouth',
    'prn': 'pro re nata (as needed)',
    'pt': 'physical therapy',
    'ptsd': 'post-traumatic stress disorder',
    'px': 'prognosis',
    'rbc': 'red blood cell',
    'ra': 'rheumatoid arthritis',
    'rr': 'respiratory rate',
    'rx': 'prescription',
    'sob': 'shortness of breath',
    'spo2': 'saturation of peripheral oxygen',
    'stat': 'immediately',
    't2dm': 'type 2 diabetes mellitus',
    'tsh': 'thyroid stimulating hormone',
    'ua': 'urinalysis',
    'uti': 'urinary tract infection',
    'wbc': 'white blood cell',
    'wnl': 'within normal limits',
    'xrt': 'radiotherapy',
    'yob': 'year of birth',
    'abd': 'abdomen',
    'adb': 'abdominal',
    'ausc': 'auscultation',
    'b/i': 'blunt injury',
    'bpd': 'bronchopulmonary dysplasia',
    'brady': 'bradycardia',
    'ctn': 'cardiac troponin',
    'dcis': 'ductal carcinoma in situ',
    'dv': 'daily value',
    'hla': 'human leukocyte antigen',
    'n/e': 'no evidence',
    'nlm': 'normal',
    't4': 'thyroxine',
    'tr': 'tricuspid regurgitation',
    'tv': 'tidal volume',
    'us': 'ultrasound',
    'vs': 'vital signs',
    'vr': 'valvular regurgitation',
}


df_abbreviations = pd.DataFrame(list(unique_abbreviations.items()), columns=['abbrevation', 'full_form'])


csv_file_path = '/content/drive/MyDrive/NLP/medical_abbrevations.csv'
df_abbreviations.to_csv(csv_file_path, index=False)

import pandas as pd
reports = pd.read_csv("/content/drive/MyDrive/NLP/Healthcare Documentation Database.csv")
print(reports)
terms = pd.read_csv("/content/drive/MyDrive/NLP/medlineplus_medical_terms.csv")
print(terms)
abbrevations = pd.read_csv("/content/drive/MyDrive/NLP/medical_abbrevations.csv")
print(abbrevations)

"""Pre Processing Medical terms dataset

"""

abbrevations = pd.read_csv("/content/drive/MyDrive/NLP/medical_abbrevations.csv")
abbrevations.head()
terms.head()

import pandas as pd
import re


terms = pd.read_csv("/content/drive/MyDrive/NLP/medlineplus_medical_terms.csv")
abbrevations = pd.read_csv("/content/drive/MyDrive/NLP/medical_abbrevations.csv")

def clean_term(term):

    term = term.lower()
    term = re.sub(r'[^a-zA-Z0-9\s]', '', term)
    term = re.sub(r'\s+', ' ', term).strip()

    for index, row in abbrevations.iterrows():
        abbrev = row['abbrevation']
        full_form = row['full_form']
        term = term.replace(abbrev, full_form)
    return term


terms['Medical Term'] = terms['Medical Term'].apply(clean_term)


terms.dropna(subset=['Explanation'], inplace=True)

terms.to_csv('/content/drive/MyDrive/NLP/cleaned_medical_terms.csv', index=False)

print(terms)

"""Pre Processing reports Dataset

"""

import pandas as pd
import re
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def clean_text(text):

    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text


reports['processed_transcription'] = reports['transcription'].apply(clean_text)
print(reports[['processed_transcription', 'cleaned_transcription']])

reports.columns

reports.drop(columns=['Serial No', 'cleaned_transcription', 'description'], inplace=True)

reports.to_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv", index=False)

import pandas as pd
import re
import nltk

reports = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv")

nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize

def tokenize(text):
   tokens = word_tokenize(text)
   return tokens
reports['tokenized_transcription'] = reports['processed_transcription'].apply(tokenize)


print(reports[['processed_transcription', 'tokenized_transcription']])
reports.to_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv", index=False)

reports.to_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv", index=False)

reports.head()

"""NER MODEL"""

import pandas as pd
reports = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv")
terms = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_medical_terms.csv")
reports.head()

medical_terms_set = set(terms['Medical Term'].str.lower())
def label_medical_terms(tokens):
    labels = []
    for token in tokens:
        if token in medical_terms_set:
            labels.append('B-MED')
        else:
            labels.append('O')
    return labels

reports['ner_labels'] = reports['tokenized_transcription'].apply(lambda x: label_medical_terms(x))


reports[['tokenized_transcription', 'ner_labels']].head()

b_med_tokens = []
for index, row in reports.iterrows():
    tokens = row['tokenized_transcription']
    labels = row['ner_labels']
    for token, label in zip(tokens, labels):
        if label == 'B-MED':
            b_med_tokens.append(token)


print("Unique B-MED tokens:", set(b_med_tokens))

"""Matching transcriptions and term explanations using semantic search
* creating embeddings for medical terms and explanations, transcriptions
* finding similarity between embeddings and matching the transcriptions with similar explanations

"""

!pip install huggingface
!pip install sentence-transformers
!pip install torch

import pandas as pd
reports = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv")
terms = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_medical_terms.csv")

terms['combined_text'] = terms['Medical Term'] + ": " + terms['Explanation']

terms.to_csv("/content/drive/MyDrive/NLP/cleaned1_medical_terms.csv", index=False)
terms.head()

import torch
from sentence_transformers import SentenceTransformer
import pickle

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

terms['embedding'] = terms['combined_text'].apply(lambda x: model.encode(x, convert_to_tensor=True))
term_embeddings = terms['embedding']

with open('/content/drive/MyDrive/NLP/term-embeddings.pkl', "wb") as fOut:
    pickle.dump(term_embeddings, fOut)

reports['embedding'] = reports['processed_transcription'].apply(lambda x: model.encode(x, convert_to_tensor=True))

reports_embeddings = reports['embedding']

with open('/content/drive/MyDrive/NLP/reports-embeddings.pkl', "wb") as fOut:
    pickle.dump(reports_embeddings, fOut)

terms.to_csv("/content/drive/MyDrive/NLP/cleaned_medical_terms.csv", index=False)
reports.to_csv("/content/drive/MyDrive/NLP/cleaned_healthcare_reports.csv", index=False)

terms.columns

"""loading embeddings"""

with open('/content/drive/MyDrive/NLP/term-embeddings.pkl', "rb") as fIn:
    term_embeddings = pickle.load(fIn)

with open('/content/drive/MyDrive/NLP/reports-embeddings.pkl', "rb") as fIn:
    reports_embeddings = pickle.load(fIn)

from sentence_transformers import util

def find_best_matches(transcription, terms_df, model, top_k=5):

    transcription_embedding = model.encode(transcription, convert_to_tensor=True)

    hits = util.semantic_search(transcription_embedding, list(terms_df['embedding']), top_k=top_k)

    if hits and len(hits) > 0:
        best_match_indices = hits[0]
        best_match_indices = [match['corpus_id'] for match in best_match_indices]
        best_matches = terms_df.iloc[best_match_indices][['Medical Term', 'Explanation']].values.tolist()

        return best_matches[0]

    else:
      return None

reports['best_matches'] = reports['processed_transcription'].apply(lambda x: find_best_matches(x, terms, model))


def extract_best_matches(x):
    if x is not None:
        return pd.Series(x)
    else:
        return pd.Series([None, None])


reports[['best_medical_term', 'best_explanation']] = reports['best_matches'].apply(extract_best_matches)


reports.to_csv("/content/drive/MyDrive/NLP/cleaned1_healthcare_reports.csv", index=False)

reports.head()

"""tokenizing using biobert model

"""

from transformers import BertTokenizer, BertForTokenClassification
import torch


tokenizer = BertTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = BertForTokenClassification.from_pretrained("dmis-lab/biobert-v1.1")

import pandas as pd
reports = pd.read_csv("/content/drive/MyDrive/NLP/cleaned1_healthcare_reports.csv")
terms = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_medical_terms.csv")

reports.shape

def preprocess_transcription(transcription):

    inputs = tokenizer(transcription, return_tensors="pt", truncation=True, padding=True)
    return inputs
def predict_entities(inputs):
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=2)
    return predictions
def decode_predictions(predictions, inputs):
    predicted_labels = []
    for i, prediction in enumerate(predictions[0]):
        label = model.config.id2label[prediction.item()]
        predicted_labels.append(label)
    return predicted_labels
def extract_entities(transcription):
    inputs = preprocess_transcription(transcription)
    predictions = predict_entities(inputs)
    entities = decode_predictions(predictions, inputs)
    return entities

reports['extracted_entities'] = reports['transcription'].apply(extract_entities)


extracted_entities = reports['extracted_entities'].apply(pd.Series)

extracted_entities.columns = [f'entity_{i+1}' for i in range(extracted_entities.shape[1])]


reports = pd.concat([reports, extracted_entities], axis=1)
reports.to_csv("/content/drive/MyDrive/NLP/cleaned2_healthcare_reports.csv", index=False)

extracted_entities.shape

extracted_entities.to_csv("/content/drive/MyDrive/NLP/entities_reports.csv", index=False)

import pandas as pd
extracted_entities = pd.read_csv("/content/drive/MyDrive/NLP/entities_reports.csv")
reports = pd.read_csv("/content/drive/MyDrive/NLP/cleaned2_healthcare_reports.csv")
terms = pd.read_csv("/content/drive/MyDrive/NLP/cleaned_medical_terms.csv")

print(reports['extracted_entities'])

tokenizer = BertTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = BertForTokenClassification.from_pretrained("dmis-lab/biobert-v1.1")

def preprocess_transcription(transcription):

    inputs = tokenizer(transcription, return_tensors="pt", truncation=True, padding=True)
    return inputs
reports['btokens'] = reports['transcription'].apply(preprocess_transcription)

reports.shape
reports.to_csv("/content/drive/MyDrive/NLP/cleaned3_healthcare_reports.csv", index=False)

reports = pd.read_csv("/content/drive/MyDrive/NLP/cleaned3_healthcare_reports.csv")

reports['btokens']

